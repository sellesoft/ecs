$ require "common"
$ local helpers = require "graphics.Helpers"

@lpp.import "graphics/Vulkan.lh"

@lpp.import "graphics/Buffer.lh"
@lpp.import "graphics/Geo.lh"

@log.import

namespace gfx
{

/* ----------------------------------------------------------------------------
 */
Buffer Buffer::create(Vulkan& vk, const CreateParams& params)
{
  String name = resolved(params.debug_name, "unnamed"_str);

$ local cerr = helpers.defCreateErr("gfx::Buffer", "name")

  if (params.size == 0)
  {
    @cerr("cannot create gfx::Buffer of size 0");
    return nil;
  }

  if ((params.properties & MemoryProperty::LazilyAllocated) && 
      (params.properties & MemoryProperty::HostCoherent
                         | MemoryProperty::HostVisible))
  {
    @cerr("memory property LazilyAllocated is incompatible with HostVisible "
          "and HostCoherent");
    return nil;
  }

  if (params.behavior == MappingBehavior::Never &&
      (params.properties & (MemoryProperty::HostVisible
                          | MemoryProperty::HostCoherent
                          | MemoryProperty::HostCached)))
  {
    @cerr("mapping behavior Never is incompatible with memory properties ",
          "HostVisible, HostCoherent, and HostCached");
    return nil;
  }

  VkBufferUsageFlags usage = 0;
  switch (params.usage)
  {
  case Usage::UniformBuffer: usage = VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT; break;
  case Usage::StorageBuffer: usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT; break;
  case Usage::IndexBuffer: usage = VK_BUFFER_USAGE_INDEX_BUFFER_BIT; break;
  case Usage::VertexBuffer: usage = VK_BUFFER_USAGE_VERTEX_BUFFER_BIT; break;
  }

  if (params.behavior == MappingBehavior::Never)
    usage |= VK_BUFFER_USAGE_TRANSFER_DST_BIT;

  VkBuffer handle;
  if (!vk.createVkBuffer(&handle, params.size, usage, name))
    return nil;

  auto failsafe_destroy_buffer = deferWithCancel
  {
    vk.destroyVkBuffer(handle);
  };

  VkMemoryPropertyFlags properties = 0;
$ local function mapprop(x, y)
  if (params.properties & MemoryProperty::$(x))
    properties |= VK_MEMORY_PROPERTY_$(y)_BIT;
$ end
  @mapprop(DeviceLocal,     DEVICE_LOCAL);
  @mapprop(HostVisible,     HOST_VISIBLE);
  @mapprop(HostCoherent,    HOST_COHERENT);
  @mapprop(HostCached,      HOST_CACHED);
  @mapprop(LazilyAllocated, LAZILY_ALLOCATED);

  VkMemoryRequirements memreq;
  vkGetBufferMemoryRequirements(vk.device, handle, &memreq);

  DeviceHeapAllocation* ptr = 
    vk.allocateAndBindVkBufferMemory(handle, properties, memreq);

  if (ptr == nullptr)
  {
    @cerr("failed to allocate and bind memory for VkBuffer");
    return nil;
  }

  auto failsafe_deallocate = deferWithCancel
  {
    vk.deallocate(ptr);
  };

  void* mapped_data = nullptr;

  switch (params.behavior)
  {
  case MappingBehavior::Never:
    if (params.data == nullptr)
    {
      @log.warn(gfx,
        "request to allocate a gfx::Buffer with mapping behavior Never. This "
        "type of request is mainly useful for intermediate compute shader "
        "memory, but compute shaders are not set up yet.\n");
    }
    else
    {
      if (!vk.stageVkBufferMemory(
            params.data,
            params.size,
            handle,
            memreq.size))
      {
        @cerr("failed to stage memory for gfx::Buffer");
        return nil;
      }
    }
    break;

  case MappingBehavior::Occasional:
    if (params.data != nullptr)
    {
      if (!vk.mapCopyAndFlushVkBufferMemory(
            params.data,
            ptr->aligned_offset,
            params.size,
            ptr))
      {
        @cerr("failed to map/copy/flush initial data for gfx::Buffer");
        return nil;
      }
    }
    break;

  case MappingBehavior::Persistent:
    {
      if (!vk.mapVkBuffer(
            &mapped_data, 
            ptr->aligned_offset,
            ptr))
      {
        @cerr("failed to map persistent gfx::Buffer");
        return nil;
      }

      if (params.data != nullptr)
      {
        mem::copy(mapped_data, params.data, params.size);
        vk.flushMappedVkBuffer(
            ptr->aligned_offset, 
            VK_WHOLE_SIZE, 
            ptr);
      }
    }
    break;
  }

  DeviceBuffer* internal_buffer = vk.buffer_pool.add();
  internal_buffer->handle = handle;
  internal_buffer->ptr = ptr;

  if (params.usage == Buffer::UniformBuffer)
  {
    if (!vk.allocateVkDescriptorSet(
          &internal_buffer->set,
          &vk.default_ubo_set_layout,
          name))
    {
      @cerr("failed to allocate descriptor set");
      return nil;
    }

    VkDescriptorBufferInfo descriptor_info = 
    {
      .buffer = internal_buffer->handle,
      .offset = 0,
      .range = params.size,
    };

    vk.updateVkDescriptorSet_Buffer(
      internal_buffer->set,
      VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
      0,
      0,
      makeSlice(&descriptor_info, 1));
  }

  failsafe_destroy_buffer.cancel();
  failsafe_deallocate.cancel();

  // The handle is our internal representation instead of the VkBuffer, as 
  // we have to track its device memory internally.
  Buffer buffer = {internal_buffer, mapped_data};
  @log.debug(gfx, "created ", buffer, " (", internal_buffer->set, ")", "\n");
  return buffer;

}

/* ----------------------------------------------------------------------------
 */
void Buffer::destroy(Vulkan& vk)
{
  if (isnil(*this))
    return;

  @log.debug(gfx, "destroying ", *this, "\n");

  auto* internal_buffer = (DeviceBuffer*)handle;

  if (internal_buffer->set != VK_NULL_HANDLE)
  {
    vk.deallocateVkDescriptorSet(internal_buffer->set);
    internal_buffer->set = VK_NULL_HANDLE;
  }

  // Unmapping when this is not mapped is safe and handled in this call.
  vk.unmapVkBuffer(internal_buffer->ptr);
  vk.destroyVkBuffer(internal_buffer->handle);
  vk.deallocate(internal_buffer->ptr);

  vk.buffer_pool.remove(internal_buffer);
  handle = nullptr;
}

/* ----------------------------------------------------------------------------
 */
void* Buffer::map(Vulkan& vk)
{
  if (isnil(*this))
  {
    @log.error(gfx, "attempt to map a nil gfx::Buffer\n");
    return nullptr;
  }

  auto* internal_buffer = (DeviceBuffer*)handle;

  if (!vk.mapVkBuffer(
      &mapped_data, 
      internal_buffer->ptr->aligned_offset,
      internal_buffer->ptr))
    return nullptr;

  return mapped_data;
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::flush(Vulkan& vk, u64 offset, u64 size)
{
  if (isnil(*this))
    return @log.error(gfx, "attempt to flush a nil gfx::Buffer\n");

  auto* internal_buffer = (DeviceBuffer*)handle;

  DeviceHeapAllocation* ptr = internal_buffer->ptr;

  VkDeviceSize dev_size = size;
  VkDeviceSize dev_offset = ptr->aligned_offset + offset;

  if (size == FLUSH_WHOLE_BUFFER)
    dev_size = ptr->aligned_size;

  // NOTE(sushi) size and offset are aligned inside this call.
  return vk.flushMappedVkBuffer(dev_offset, dev_size, internal_buffer->ptr);
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::copyAndFlush(Vulkan& vk, void* data, u64 size)
{
  if (isnil(*this))
    return @log.error(gfx, "attempt to copy and flush a nil gfx::Buffer\n");

  if (mapped_data == nullptr)
    return @log.error(gfx, "attempt to copy and flush unmapped ", *this, "\n");

  auto* internal_buffer = (DeviceBuffer*)handle;
  DeviceHeapAllocation* ptr = internal_buffer->ptr;

  mem::copy(mapped_data, data, size);

  VkDeviceSize dev_size = size;
  VkDeviceSize dev_offset = ptr->aligned_offset;

  if (size == FLUSH_WHOLE_BUFFER)
    dev_size = VK_WHOLE_SIZE;
  else 
    dev_size = alignUp(size, vk.getHeap(ptr)->alignment);

  return vk.flushMappedVkBuffer(dev_offset, dev_size, ptr);
}

/* ----------------------------------------------------------------------------
 */
void Buffer::unmap(Vulkan& vk)
{
  if (isnil(*this))
  {
    @log.error(gfx, "attempt to unmap a nil gfx::Buffer\n");
    return;
  }

  auto* internal_buffer = (DeviceBuffer*)handle;

  if (!vk.unmapVkBuffer(internal_buffer->ptr))
    @log.error(gfx, 
      "attempt to unmap ", *this, " but it was already unmapped"); 
}

}
